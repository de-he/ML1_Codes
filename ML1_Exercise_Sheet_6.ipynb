{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning I, Programming Exercise 6\n",
    "You will probably find that the tasks in this notebook are considerably more difficult than those in the previous mandatory coding exercises. That is because it will examine new concepts that are only touched upon briefly in the lecture. If you get stuck at some point or find that the task descriptions are unclear, please ask your questions in the \"Exercise Questions\" channel of the Mattermost and tag it with `@tmichels` so that I will be notified.\n",
    "\n",
    "If you haven't worked with PyTorch yet, you might want to check out the solution for the previous programming exercise, as it contains a brief introduction to PyTorch and the convolution operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Classifiers (OPTIONAL, but it might be helpful to read the exercise text)\n",
    "In this exercise, we want to implement a simple neural network classifier in PyTorch. In order to visualize similarities and/or differences to the previous learning models (SVM, kNN, etc.), we will apply our classifier to the same toy datasets. However, we will consider the multi-class case with $k \\geq 2$ different classes right from the start. More formally, we define a neural network for classification as a function $f: \\mathcal{X} \\to \\mathbb{R}^k$ and we predict a class label $\\hat{y} \\in \\mathcal{Y} = \\{1, \\ldots, k\\}$ to an input $\\mathbf{x} \\in \\mathcal{X}$ in the following way:\n",
    "\\begin{equation*}\n",
    "    \\hat{y} = \\operatorname{arg\\ max}_{y \\in \\{1, \\ldots, k\\}} f(\\mathbf{x})_y,\n",
    "\\end{equation*}\n",
    "that is we consider the class with maximal *score*. \n",
    "\n",
    "As you have learned in the lecture already, neural networks usually don't use the hinge loss for training, but rather the log loss which is defined as\n",
    "\\begin{equation*}\n",
    "    l(f(\\mathbf{x}), y) = \\log\\left(1 + e^{-yf(\\mathbf{x})}\\right).\n",
    "\\end{equation*}\n",
    "However, using the log loss still makes sense only in the binary classification setting, since it requires that the output of $f$ is scalar. Luckily, one can extend the binary log loss to the case where the number of classes $k\\geq2$:\n",
    "\\begin{equation*}\n",
    "    \\ell(f(\\mathbf{x}), y) = -\\log(\\operatorname{softmax}(f(\\mathbf{x}))_y),\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "    \\operatorname{softmax}(\\mathbf{z})_i := \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}}\n",
    "\\end{equation*}\n",
    "and $y \\in \\mathcal{Y}$ is the true label for input $\\mathbf{x} \\in \\mathcal{X}$. This loss is usually called *cross entropy* loss and its functionality is implemented in the [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) module.\n",
    "\n",
    "It is not straightforward to see that $\\ell$ is really an extension to the log loss from the lecture, so let's have a closer look at the case where $k=2$. Contrary to the lecture, we assume class labels $y \\in \\{1, 2\\}$ instead of $\\tilde{y} \\in \\{-1, 1\\}$. First, we examine the softmax function for $\\mathbf{z} = f(\\mathbf{x}) \\in \\mathbb{R}^2$:\n",
    "\\begin{equation*}\n",
    "    \\operatorname{softmax}(\\mathbf{z})_1 = \\frac{e^{z_1}}{e^{z_1} + e^{z_2}} = \\frac{e^{z_1}}{e^{z_1}(1 + e^{z_2-z_1})} = \\frac{1}{1 + e^{-(z_1-z_2)}} = \\sigma(z_1 - z_2)\n",
    "\\end{equation*}\n",
    "and by the same calculation\n",
    "\\begin{equation*}\n",
    "    \\operatorname{softmax}(\\mathbf{z})_2 = \\sigma(z_2 - z_1).\n",
    "\\end{equation*}\n",
    "Now consider the case that the true label $y=1$. We can directly calculate the cross entropy loss as\n",
    "\\begin{align*}\n",
    "    \\ell(\\mathbf{z}, y) &= -\\log(\\operatorname{softmax}(\\mathbf{z})_1)\\\\\n",
    "        &= -\\log(\\sigma(z_1-z_2))\\\\\n",
    "        &= -\\log\\left(\\frac{1}{1 + e^{-(z_1-z_2)}}\\right)\\\\\n",
    "        &= \\log\\left(1 + e^{-(z_1-z_2)}\\right)\\\\\n",
    "        &= \\log\\left(1 + e^{-\\tilde{y}(z_1-z_2)}\\right)\n",
    "\\end{align*}\n",
    "for $\\tilde{y}=1$. We can also consider the case $y=2$, for which we get by the same calculation that\n",
    "\\begin{equation*}\n",
    "    \\ell(\\mathbf{z}, y) = \\log\\left(1 + e^{-\\tilde{y}(z_1-z_2)}\\right)\n",
    "\\end{equation*}\n",
    "for $\\tilde{y}=-1$. Hence, we conclude that the cross entropy loss is indeed an extension of the log loss from the lecture. Note that $\\mathbf{z} = f(\\mathbf{x}) \\in \\mathbb{R}^k$ is called the *logit* vector in most of the literature.\n",
    "\n",
    "Well, that should be enough theory for a programming exercise, so let's get started with implementing what we just learned:\n",
    "1. Implement a fully connected neural network with a specified number of hidden layers of and each with a specified number of neurons. That is, the network class should accept a parameter `hidden_layers`, which specifies both. For example, `hidden_layers=5` should produce a network with one hidden layer that contains 5 neurons. `hidden_layers=[10, 5]` on the other hand would produce two hidden layers, the first of which has 10 neurons and a second one with 5.\n",
    "\n",
    "   In the PyTorch framework, this is usually achievd by subclassing [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#module) and overriding the `forward` method. This method should define how a network input is to be transformed into an output and you can use any of the layers that you have defined in the class' constructor. It should also be possible to pass an activation function to the module which is then used between each fully-connected layer. The output of the final layer should not be passed through an activation function.\n",
    "   \n",
    "   **Hint:** You might want to have a look at [`torch.nn.ModuleList`](https://pytorch.org/docs/stable/nn.html#modulelist).\n",
    "\n",
    "\n",
    "2. PyTorch does not offer any convenience functions for its modules, such as `fit(X, y)` for training on some dataset. Therefore, you'll need to write your own functions for training the model on a given dataset or applying the network to previously unseen data, if you want to use the PyTorch network with the same API as the Scikit-Learn models from earlier exercises. However, unlike with the SVMs that you have implemented from scratch, there is no need to manually compute any gradients, since they can be computed automatically by PyTorch's `autograd` engine which, in fact, implements an extended version of the backpropagation algorithm from the lecture. \n",
    "\n",
    "   So, in order to make your PyTorch networks work with the Scikit-Learn API, write a wrapper class that takes an existing PyTorch network and offers the functions `fit(X, y)` for training the network and `predict(X)` for applying the network to previously unseen data. `X` and `y` should be Numpy arrays as usual.\n",
    "   \n",
    "   Training should be done via mini-batch stochastic gradient descent, see [`torch.optim.SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) for an implementation that you can use. However, instead of sampling a random mini-batch before each update step, it is much more common in neural network training to randomly shuffle the entire dataset before the actual training loop and then process the shuffled data deterministically from start to end. One pass through the entire dataset is called an *epoch* and training is usually not limited by a fixed amount of gradient descent updates but by the number of epochs to perform instead. Note that the step size of the (stochastic) gradient descent algorithm is often called *learning rate* in the context of neural network training.\n",
    "\n",
    "\n",
    "3. Train your neural network on the toy dataset(s) generated in the code skeleton for a sufficient number of epochs (about 500) and visualize its decision boundary for different activation functions and hidden layer configurations (number of neurons and number of layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x0 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, activation=None, num_classes=10, in_features=2, hidden_layers=10):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        # TODO: initialise the network's layers here\n",
    "        raise NotImplementedError('TODO')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Apply the layers and activation to the input\n",
    "        raise NotImplementedError('TODO')\n",
    "\n",
    "class TorchClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, torch_model, loss, optimizer_class, device, step_size=0.001, \n",
    "                 batch_size=64, epochs=20):\n",
    "        super(TorchClassifier, self).__init__()\n",
    "        \n",
    "        raise NotImplementedError('TODO')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # TODO: Train the network on the given data\n",
    "        raise NotImplementedError('TODO')\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: Predict labels for X using the trained network\n",
    "        raise NotImplementedError('TODO')\n",
    "            \n",
    "\n",
    "# Generate toy data\n",
    "n = 200\n",
    "n_classes = 2\n",
    "X, Y = make_moons(n, noise=0.3, random_state=123456)\n",
    "\n",
    "#n_classes = 2\n",
    "#X, Y = make_classification(n, n_features=2, n_redundant=0, n_classes=n_classes,\n",
    "#                           n_clusters_per_class=2, class_sep=1.2, random_state=12345)\n",
    "# You can also try a dataset with more than 2 classes\n",
    "#n_classes = 3\n",
    "#X, Y = make_classification(n, n_features=2, n_redundant=0, n_classes=n_classes,\n",
    "#                           n_clusters_per_class=1, class_sep=1.0, random_state=5432)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=12345)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD\n",
    "# Change this to 'cuda' if you can and want to use a GPU for training\n",
    "# Otherwise, you can completely ignore this parameter\n",
    "device = torch.device('cpu')  \n",
    "batch_size=100\n",
    "epochs = 500\n",
    "def get_classifier(activation=None, hidden_layers=10, step_size=0.001):\n",
    "    return TorchClassifier(SimpleNet(activation=activation, hidden_layers=hidden_layers, num_classes=n_classes), \n",
    "                           loss, optimizer, device, step_size=step_size, batch_size=batch_size, epochs=epochs)\n",
    "classifiers = [\n",
    "    # Add some instances of the neural network here, for example:\n",
    "    # ('Activation: ReLU, Hidden Neurons: 10, 10',\n",
    "    #  get_classifier(torch.nn.ReLU(), [10, 10], 0.5)),\n",
    "]\n",
    "\n",
    "# Plot decision surface\n",
    "# First generate grid\n",
    "res = 200  # Resolution of the grid in cells\n",
    "x_max, y_max = np.max(X, axis=0)\n",
    "x_min, y_min = np.min(X, axis=0)\n",
    "x_min, x_max = x_min - (x_max - x_min) * 0.0625, x_max + (x_max - x_min) * 0.0625\n",
    "y_min, y_max = y_min - (y_max - y_min) * 0.0625, y_max + (y_max - y_min) * 0.0625\n",
    "grid_x, grid_y = np.meshgrid(np.linspace(x_min, x_max, res),\n",
    "                             np.linspace(y_min, y_max, res))\n",
    "# Get test array from grid\n",
    "grid_input = np.c_[grid_x.reshape(-1), grid_y.reshape(-1)]\n",
    "\n",
    "cmap = colors.ListedColormap([[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n",
    "rows = (len(classifiers)+1)//2\n",
    "fig, axes = plt.subplots(rows, 2, sharex=True, sharey=True, figsize=(12, 4.5*rows))\n",
    "for (name, clf), ax in zip(classifiers, axes.ravel()):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    score = clf.score(X_test, Y_test)\n",
    "    train_score = clf.score(X_train, Y_train)\n",
    "    grid_out = clf.predict(grid_input).reshape(grid_x.shape)\n",
    "\n",
    "    ax.set_title('%s,\\nTrain. Acc.: %.3f, Val. Acc.: %.3f' % (name, train_score, score))\n",
    "    ax.contourf(grid_x, grid_y, grid_out, alpha=0.5, cmap=plt.cm.brg)\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap, edgecolor='k')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LeNet (MANDATORY)\n",
    "The *LeNet-5* architecture, proposed in 1998 by Yann LeCun et al., was one of the first CNNs and can be seen as the predecessor and source of inspiration of most modern CNN architectures. Most beginner-level tutorials on CNNs include the implementation of a \"LeNet-like\" network (for example the [official PyTorch tutorials](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)), which differs from the original in quite some ways. In this exercise, however, we would like to re-implement the LeNet-5 architecture as close to the original as it is reasonably possible in a modern framework such as PyTorch. Please read the relevant sections on LeNet-5 from the [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) by LeCun et al. and implement their architecture with two small changes:\n",
    "* You don't have to reproduce the complicated connection pattern between feature maps in layer *C3*. But in order to stay faithful to the spirit of original, implement the layer such that half of the filters are connected only to the first three input feature maps and the other half to the last three.\n",
    "* The output layer (called RBF layer in the paper) with fixed parameters is not too useful if you want to use the network on different datasets. Replace it by a standard fully-connected layer that has as many outputs as there are classes for the specific task.\n",
    "\n",
    "**Hints:**\n",
    "* Carefully read the documentation of the [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) module to understand what the different parameters of a convolution layer are and what their effect is. You can also check the previous programming exercise for an introduction to convolutions.\n",
    "* When you put everything together as described in the paper and the two bullet points above, your model should have seven layers and a total of 60550 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network parameters: 60550\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self, activation=None, num_classes=10, in_channels=3):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        # TODO: initialise the network's layers here\n",
    "        pool = torch.nn.AvgPool2d(2, 2)\n",
    "        self.C1 = torch.nn.Conv2d(in_channels, 6, 5, padding=2)\n",
    "        self.S2 = torch.nn.Sequential(pool, torch.nn.Conv2d(6, 6, 1, groups=6))\n",
    "        self.C3 = torch.nn.Conv2d(6, 16, 5, groups=2)\n",
    "        self.S4 = torch.nn.Sequential(pool, torch.nn.Conv2d(16, 16, 1, groups=16))\n",
    "        self.C5 = torch.nn.Conv2d(16, 120, 5)\n",
    "        self.F6 = torch.nn.Linear(120, 84)\n",
    "        self.out = torch.nn.Linear(84, num_classes) ##ok\n",
    "        #raise NotImplementedError('TODO')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Apply the layers and activation to the input\n",
    "        x = self.activation(self.C1(x))\n",
    "        x = self.activation(self.S2(x))\n",
    "        x = self.activation(self.C3(x))\n",
    "        x = self.activation(self.S4(x))\n",
    "        x = self.activation(self.C5(x))\n",
    "        x = self.activation(self.F6(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "        #raise NotImplementedError('TODO')\n",
    "\n",
    "# TODO: Choose the activation function from the paper\n",
    "activation = lambda x: 1.7159*torch.tanh(x)\n",
    "network = LeNet(activation=activation, in_channels=1)\n",
    "print('Network parameters:', sum(p.numel() for p in network.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ANN Training (MANDATORY)\n",
    "In this exercise, we want to use the CNN from part 2 on the so called \"[Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)\" dataset. It comes with a training set that contains 60000 gray-scale images of size 28 by 28 pixels and a test set with 10000 images of the same dimensions. Each image belongs to exactly one of the ten classes and each class corresponds to a different piece of clothing (hence the name). Fashion MNIST is intended as a replacement for the original MNIST dataset which contained images of handwritten digits from 0 to 9 with the same dimensionality. As you have probably read in the paper by LeCun et al., the LeNet-5 architecture was originally used for recognizing such handwritten digits.\n",
    "\n",
    "You don't need to download the dataset manually, since this functionality is already included in the `torchvision` package, please take a look at [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/stable/datasets.html#fashion-mnist). However, this class won't load all the images into memory on its own, instead data is only streamed from disk at the moment where you access it. Note that this is actually desired behaviour, since some datasets might not even fit in your main memory, so you would ideally only keep the mini-batch that you are currently processing in memory. This kind of functionailty is implemented in the [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class. It creates an iterator which loops over a shuffled version of the given dataset and returns a `(batch_X, batch_y)` tuple with each iteration. One pass through the iterator corresponds to one epoch of training.\n",
    "\n",
    "1. Training a neural network can take a considerable amount of time, so it is important to keep track of what is going on during training. For that, write a new training function that trains a neural network using a `DataLoader` as described above. As mentioned in ex. 1, training should be done via mini-batch stochastic gradient descent; see [`torch.optim.SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) for an implementation that you can use. However, instead of sampling a random mini-batch before each update step, it is much more common in neural network training to randomly shuffle the entire dataset before the actual training loop and then partition the shuffled data deterministically into mini-batches from start to end. One pass through the entire dataset is called an *epoch* and training is usually not limited by a fixed amount of gradient descent updates but by the number of epochs to perform instead. Note that the step size of the (stochastic) gradient descent algorithm is often called *learning rate* in the context of neural network training. Most of that functionality is already included in the `DataLoader` or `torch.optim.SGD` classes, please consult their documentation for more details.\n",
    "\n",
    "   Before training, the dataset is usually split into a training and test dataset, or -- like it is the case for FashionMNIST -- the dataset already contains distinct training and test samples. Report the average loss and accuracy (fraction of correctly classified inputs) on the training dataset during each epoch and also compute average loss and accuracy on the test set after each epoch. Your function should return a collection of those values, so that they can be visualized later.\n",
    "\n",
    "   **Tip:** The `tqdm` package makes it easy to create good-looking progress bars that you can customize with your own texts. To make it work in a Jupyter notebook, you should use the classes from the [`tqdm.notebook`](https://tqdm.github.io/docs/notebook/) module.\n",
    "\n",
    "\n",
    "2. Train the LeNet network from ex. 2 on the Fashion MNIST dataset for 15 epochs using the function from part 1 with the following parameters:\n",
    "   * batch size: 64\n",
    "   * step_size: 0.01\n",
    "   * 2-norm regularization coefficient (also called \"weight decay\"): 0.0004 \n",
    "   * loss function: [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) (see ex. 1 for details)\n",
    "   \n",
    "   Your Network should reach at least 86% accuracy on the test set in one of the 15 epochs.\n",
    "   \n",
    "   Then use the data returned by the training function to visualize the training procedure. More specifically, create two plots: One should show the average loss on the y-axis and the epoch on the x-axis. Plot the values for training and testing set in the same figure, so that you can easily compare them. You should do the same for the accuracy in each epoch, but in a separate plot.\n",
    "\n",
    "   **Tip:** Take a look at the [`torch.save`](https://pytorch.org/docs/stable/generated/torch.save.html) function which allows you to save your model after or during training. It might become annoying to re-train the model each time you change a line of code, so you can train it just once and then simply load it from disk afterwards.\n",
    "\n",
    "\n",
    "3. Given a dataset $D = \\{(\\mathbf{x}^{(1)}, y^{(1)}), (\\mathbf{x}^{(2)}, y^{(2)}), \\ldots, (\\mathbf{x}^{(n)}, y^{(n)})\\} \\subseteq \\mathcal{X} \\times \\{1, \\ldots, k\\}$ and a classifier $f: \\mathcal{X} \\to \\{1, \\ldots, k\\}$, we define the confusion matrix $C_f$ in the following way:\n",
    "\\begin{equation*}\n",
    "    C_{ij} := \\frac{\\lvert \\left\\{ (\\mathbf{x}, y) \\in D \\mid y=i \\land f(\\mathbf{x})=j \\right\\} \\rvert}{\\lvert \\left\\{ (\\mathbf{x}, y) \\in D \\mid y=i \\right\\} \\rvert},\n",
    "\\end{equation*}\n",
    "i.e., $C_{ij}$ is defined as the fraction of data points from $D$ that belong to class $i$ and are classified as $j$ by $f$. Such a matrix offers more information than a simple scalar like accuracy or loss, since it becomes obvious where exactly the classifier makes its mistakes.\n",
    "  \n",
    "   After training the LeNet on the Fashion MNIST training dataset, compute, show and interpret the confusion matrix of that classifier on the Fashion MNIST test dataset. You can also compute the confusion matrix after each epoch of training to see how it develops. A mapping from numeric class labels to semantically meaningful class names is available on the [Fashion MNIST website](https://github.com/zalandoresearch/fashion-mnist#labels).\n",
    "   \n",
    "\n",
    "4. **(OPTIONAL)** Start experimenting with hyperparameters such as learning rate, batch size, and/or optimization algorithm. Can you improve the test set accuracy compared to the original set of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (7680x1 and 120x84)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7603963040cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;31m#### TODO -> Move train function in else (see TODO)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoints\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mload_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-7603963040cf>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(network, loss, optimizer, train_iter, val_iter, num_epochs, device, start_epoch, checkpoints, out_dir)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mcurrent_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-22705fcf97ec>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mF6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (7680x1 and 120x84)"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(network, loss, optimizer, train_iter, val_iter, num_epochs, device='cpu', start_epoch=0,\n",
    "                checkpoints=False, out_dir=None):\n",
    "    \"\"\"\n",
    "    Train a PyTorch neural network with the specified parameters\n",
    "    \n",
    "    :param network: The neural network to train. Should be a subclass of :class:`torch.nn.Module`\n",
    "    :param loss: The loss function to use. Should take arguments of the form (prediction, true_value)\n",
    "    :param optimizer: The optimization algorithm that is used for training. Should be a subclass of :class:`torch.optim.Optimizer`\n",
    "    :param train_iter: A :class:`torch.utils.data.Dataloader` instance that yields mini-batches for training\n",
    "    :param val_iter: A :class:`torch.utils.data.Dataloader` instance that yields mini-batches for testing\n",
    "    :param num_epochs: The number of epochs to train the network for\n",
    "    :param device: The device to use for training. Usually 'cpu' or 'cuda'\n",
    "    :param start_epoch: The first epoch to start with. This is for the case of resuming training from a checkpoint\n",
    "    :param checkpoints: Whether to save a checkpoint of the model after each training epoch\n",
    "    :param out_dir: Directory where the checkpoints will be saved\n",
    "    \n",
    "    :returns: The avg. training and test loss and accuracy for each epoch\n",
    "    \"\"\"\n",
    "    for i in range(num_epochs):\n",
    "        current_batch = next(iter(train_iter))\n",
    "        X, Y = current_batch\n",
    "        predictions = network(X)\n",
    "        print(predictions)\n",
    "    \n",
    "    # TODO: Train the model and return the training history\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "def plot_history(history):\n",
    "    # TODO: Plot training and validation curves for avg. loss and accuracy\n",
    "    raise NotImplementedError('TODO')\n",
    "    \n",
    "def confusion_matrix(network, val_iter, num_classes=10, device='cpu'):\n",
    "    # TODO: Compute the confusion matrix for classifier 'network' and\n",
    "    # dataset 'val_iter' (given as a DataLoader)\n",
    "    raise NotImplementedError('TODO')\n",
    "\n",
    "\n",
    "# Change this to 'cuda' if you can and want to use a GPU for training\n",
    "# Otherwise, you can completely ignore this parameter\n",
    "device = torch.device('cpu')\n",
    "dataset = torchvision.datasets.FashionMNIST\n",
    "batch_size = 64\n",
    "num_epochs = 15\n",
    "num_workers = 0  # You can use multiple processes to load the data. 0 means that everything is done in the same process\n",
    "step_size = 1e-2\n",
    "regularization=0.0004  # Contols how much the Frobenius norm of the parameters contributes to the final loss\n",
    "checkpoints = True  # Save the model after each epoch of training\n",
    "out_dir = 'models'  # Folder, where your model checkpoints will be saved\n",
    "load_path = None  # Don't train the model, but load it from the specified file instead\n",
    "data_dir = 'data'  # Folder that contains the dataset. If it is not present, \n",
    "                   # the dataset will be downloaded into that folder\n",
    "\n",
    "#Load the training and the test dataset and wrap them in a DataLoader\n",
    "train_dataset = dataset(root=data_dir, train=True, download=True, transform = torchvision.transforms.ToTensor())\n",
    "train_set =  torch.utils.data.DataLoader(torch.utils.data.BufferedShuffleDataset(train_dataset, train_dataset.__len__()), batch_size)\n",
    "\n",
    "test_dataset = dataset(root=data_dir, train = False, transform = torchvision.transforms.ToTensor())\n",
    "test_set =torch.utils.data.DataLoader(torch.utils.data.BufferedShuffleDataset(test_dataset, test_dataset.__len__()), batch_size)\n",
    "#Instantiate the network, optimizer, etc.\n",
    "activation = lambda x: 1.7159*torch.tanh(x)\n",
    "network = LeNet(activation=activation, in_channels=1) #in_channels=1 (as we deal with grayscale images)\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), step_size)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#### TODO -> Move train function in else (see TODO)\n",
    "\n",
    "train_model(network, loss_function, optimizer, train_set, test_set, num_epochs, device='cpu', start_epoch=0, checkpoints=False, out_dir=None)\n",
    "\n",
    "if load_path is not None:\n",
    "    # TODO: Load model, optimizer state and history \n",
    "    pass\n",
    "else:\n",
    "    # TODO: Otherwise, we train the model for the specified number of epochs\n",
    "    # using the train function\n",
    "    pass\n",
    "\n",
    "# TODO: Plot the training history\n",
    "# plot_history(history)\n",
    "\n",
    "# TODO: And finally compute the confusion matrix\n",
    "# mat = confusion_matrix(network, val_iter, device=device)\n",
    "# print('Confusion matrix:', mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
